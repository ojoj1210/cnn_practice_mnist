{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a0ca78-8013-4aca-9328-f1243e69ac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np # linear algebra\\nimport struct\\nfrom array import array\\nfrom os.path  import join\\n\\n#\\n# MNIST Data Loader Class\\n#\\nclass MnistDataloader(object):\\n    def __init__(self, training_images_filepath,training_labels_filepath,\\n                 test_images_filepath, test_labels_filepath):\\n        self.training_images_filepath = training_images_filepath\\n        self.training_labels_filepath = training_labels_filepath\\n        self.test_images_filepath = test_images_filepath\\n        self.test_labels_filepath = test_labels_filepath\\n    \\n    def read_images_labels(self, images_filepath, labels_filepath):        \\n        labels = []\\n        with open(labels_filepath, \\'rb\\') as file:\\n            magic, size = struct.unpack(\">II\", file.read(8))\\n            if magic != 2049:\\n                raise ValueError(\\'Magic number mismatch, expected 2049, got {}\\'.format(magic))\\n            labels = array(\"B\", file.read())        \\n        \\n        with open(images_filepath, \\'rb\\') as file:\\n            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\\n            if magic != 2051:\\n                raise ValueError(\\'Magic number mismatch, expected 2051, got {}\\'.format(magic))\\n            image_data = array(\"B\", file.read())        \\n        images = []\\n        for i in range(size):\\n            images.append([0] * rows * cols)\\n        for i in range(size):\\n            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\\n            img = img.reshape(28, 28)\\n            images[i][:] = img            \\n        \\n        return images, labels\\n            \\n    def load_data(self):\\n        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\\n        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\\n        return (x_train, y_train),(x_test, y_test)   \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based of code from: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
    "# download mnist dataset from kaggle\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "'''\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114db19c-41cd-4cb4-8c20-bf587db6c657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom PIL import Image, ImageOps\\nimport os\\n\\ndef save_image(filename, data_array):\\n    im = Image.fromarray(data_array.astype(\\'uint8\\'))\\n    im_invert = ImageOps.invert(im)\\n    im_invert.save(filename)\\n\\ninput_path = \\'C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\og\\'\\ntraining_images_filepath = join(input_path, \\'train-images-idx3-ubyte\\\\train-images-idx3-ubyte\\')\\ntraining_labels_filepath = join(input_path, \\'train-labels-idx1-ubyte\\\\train-labels-idx1-ubyte\\')\\ntest_images_filepath = join(input_path, \\'t10k-images-idx3-ubyte\\\\t10k-images-idx3-ubyte\\')\\ntest_labels_filepath = join(input_path, \\'t10k-labels-idx1-ubyte\\\\t10k-labels-idx1-ubyte\\')\\n\\nmnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\\n(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\\n\\nfor i in range(60000):\\n    DIR_NAME = \"C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\training_data\\\\\" + str(y_train[i]) \\n    filename = \"{0}/{1:05d}.jpg\".format(DIR_NAME,i)\\n    print(filename)\\n    save_image(filename, np.array(x_train[i]))\\n\\n\\nfor i in range(10000):  \\n    DIR_NAME = \"C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\test_data\\\\\" + str(y_test[i]) \\n    filename = \"{0}/{1:05d}.jpg\".format(DIR_NAME,i + 60000)\\n    print(filename)\\n    save_image(filename, np.array(x_test[i]))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some code taken from:https://gist.github.com/uchidama/40f3076283e485e75bc904193ede6929#file-mnist_to_jpg-py\n",
    "'''\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "def save_image(filename, data_array):\n",
    "    im = Image.fromarray(data_array.astype('uint8'))\n",
    "    im_invert = ImageOps.invert(im)\n",
    "    im_invert.save(filename)\n",
    "\n",
    "input_path = 'C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\og'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte\\\\train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte\\\\train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte\\\\t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte\\\\t10k-labels-idx1-ubyte')\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "for i in range(60000):\n",
    "    DIR_NAME = \"C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\training_data\\\\\" + str(y_train[i]) \n",
    "    filename = \"{0}/{1:05d}.jpg\".format(DIR_NAME,i)\n",
    "    print(filename)\n",
    "    save_image(filename, np.array(x_train[i]))\n",
    "\n",
    "\n",
    "for i in range(10000):  \n",
    "    DIR_NAME = \"C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\test_data\\\\\" + str(y_test[i]) \n",
    "    filename = \"{0}/{1:05d}.jpg\".format(DIR_NAME,i + 60000)\n",
    "    print(filename)\n",
    "    save_image(filename, np.array(x_test[i]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca082d67-6e8e-4095-84d3-d06ce1169f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b00c48a-fc98-434a-81cd-61d428a060ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8739c734-9505-45cd-ad61-1df972385c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   rotation_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\training_data',\n",
    "                                                 target_size = (28, 28),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033696a2-99e7-4491-b8da-92098bbb4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\test_data',\n",
    "                                            target_size = (28, 28),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1063ada-5948-469a-bf80-5e28df1ccb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9604660f-af49-47da-9d06-a34559526011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, activation='relu', input_shape=[28, 28, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c2295c-7210-46fc-937d-d54544b3cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1670cab-d9cd-420c-b6ad-5c4a67a2ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=4, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb490c4-7401-4366-8fa9-ce1fc8ac56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4465ed-7d8f-44ea-90f5-e3eeb72bddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7036ba68-6232-48a9-b491-938128b9f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b673f34-f9a0-4508-821d-a0d0d8145ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ff89f47-7a12-4962-8827-bb37ed34eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.7861 - loss: 0.6518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 288ms/step - accuracy: 0.7861 - loss: 0.6517 - val_accuracy: 0.9645 - val_loss: 0.1205\n",
      "Epoch 2/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9541 - loss: 0.1481 - val_accuracy: 0.9764 - val_loss: 0.0729\n",
      "Epoch 3/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9668 - loss: 0.1077 - val_accuracy: 0.9776 - val_loss: 0.0676\n",
      "Epoch 4/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9721 - loss: 0.0877 - val_accuracy: 0.9814 - val_loss: 0.0617\n",
      "Epoch 5/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9754 - loss: 0.0773 - val_accuracy: 0.9775 - val_loss: 0.0714\n",
      "Epoch 6/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9784 - loss: 0.0692 - val_accuracy: 0.9790 - val_loss: 0.0684\n",
      "Epoch 7/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9796 - loss: 0.0635 - val_accuracy: 0.9793 - val_loss: 0.0671\n",
      "Epoch 8/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9806 - loss: 0.0603 - val_accuracy: 0.9859 - val_loss: 0.0477\n",
      "Epoch 9/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9831 - loss: 0.0564 - val_accuracy: 0.9840 - val_loss: 0.0543\n",
      "Epoch 10/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9822 - loss: 0.0551 - val_accuracy: 0.9827 - val_loss: 0.0550\n",
      "Epoch 11/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9828 - loss: 0.0528 - val_accuracy: 0.9873 - val_loss: 0.0379\n",
      "Epoch 12/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9847 - loss: 0.0448 - val_accuracy: 0.9871 - val_loss: 0.0387\n",
      "Epoch 13/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 37ms/step - accuracy: 0.9846 - loss: 0.0469 - val_accuracy: 0.9894 - val_loss: 0.0385\n",
      "Epoch 14/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9847 - loss: 0.0460 - val_accuracy: 0.9869 - val_loss: 0.0425\n",
      "Epoch 15/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9861 - loss: 0.0437 - val_accuracy: 0.9864 - val_loss: 0.0442\n",
      "Epoch 16/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9876 - loss: 0.0401 - val_accuracy: 0.9854 - val_loss: 0.0448\n",
      "Epoch 17/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9859 - loss: 0.0440 - val_accuracy: 0.9824 - val_loss: 0.0659\n",
      "Epoch 18/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9880 - loss: 0.0385 - val_accuracy: 0.9880 - val_loss: 0.0442\n",
      "Epoch 19/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 37ms/step - accuracy: 0.9865 - loss: 0.0428 - val_accuracy: 0.9868 - val_loss: 0.0481\n",
      "Epoch 20/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9871 - loss: 0.0403 - val_accuracy: 0.9827 - val_loss: 0.0620\n",
      "Epoch 21/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9878 - loss: 0.0367 - val_accuracy: 0.9846 - val_loss: 0.0534\n",
      "Epoch 22/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9887 - loss: 0.0349 - val_accuracy: 0.9861 - val_loss: 0.0471\n",
      "Epoch 23/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9883 - loss: 0.0360 - val_accuracy: 0.9903 - val_loss: 0.0356\n",
      "Epoch 24/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 38ms/step - accuracy: 0.9897 - loss: 0.0321 - val_accuracy: 0.9877 - val_loss: 0.0469\n",
      "Epoch 25/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9880 - loss: 0.0352 - val_accuracy: 0.9857 - val_loss: 0.0517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27f397994f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf52ea20-9250-4660-9fe0-8b62b066bdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:\\\\Users\\\\limjo\\\\Documents\\\\work\\\\cnn_practice\\\\prediction\\\\60274.jpg', target_size = (28, 28))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
